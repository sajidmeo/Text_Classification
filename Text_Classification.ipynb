{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is taking from UCI Machine Learning repository.It contains over 5000 SMS labeled messages that have been collected for mobile phone spam research.link of dataset:https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_table(\"SMSSpamCollection\",header=None,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclass=df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we observe there is two classes of dataset, ham (total meassages=4825)and spam (total meassages=747)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data is an essential step in natural language process. In the following cells, we will convert our class labels to binary values using the LabelEncoder from sklearn, replace email addresses, URLs, phone numbers, and other symbols by using regular expressions, remove stop words, and extract word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convrting class labels into binary values,0=ham and 1=spam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "y=encoder.fit_transform(myclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varibale used for storing the sms messages data\n",
    "text_message=df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_message[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use regular exp  to replace email addess,urls...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace email adress with \"emailadder\"\n",
    "my_mess=text_message.str.replace(r\"^\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,3}$\",\"emailadder\")\n",
    "#replace Url \"webaddress\" \n",
    "my_mess=text_message.str.replace(r\"/^(?:([A-Za-z]+):)?(\\/{0,3})([0-9.\\-A-Za-z]+)(?::(\\d+))?(?:\\/([^?#]*))?(?:\\?([^#]*))?(?:#(.*))?$/;\",\"webaddress\")\n",
    "#replace number with \"number\"\n",
    "my_mess=text_message.str.replace(r\"^[0-9]\",\"number\")\n",
    "#replace money with \"moneysymbol\"\n",
    "my_mess=text_message.str.replace(r\"^\\$(\\d{1,3}(\\,\\d{3})*|(\\d+))(\\.\\d{2})?$\",\"moneysymbol\")\n",
    "#replace 10 digit phone number with \"contact\"\n",
    "my_mess=text_message.str.replace(r\"^[2-9]{2}[0-9]{8}$\",\"contact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations and white spaces\n",
    "my_mess=text_message.str.replace(r\"\\d+(\\.\\d+)?\",\" \")\n",
    "my_mess=text_message.str.replace(r\"\\s+\",\" \")\n",
    "my_mess=text_message.str.replace(r\"^\\s+?$\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point, crazy.. available only ...\n",
       "1                           ok lar... joking wif u oni...\n",
       "2       free entry in 2 a wkly comp to win fa cup fina...\n",
       "3       u dun say so early hor... u c already then say...\n",
       "4       nah i don't think he goes to usf, he lives aro...\n",
       "5       freemsg hey there darling it's been 3 week's n...\n",
       "6       even my brother is not like to speak with me. ...\n",
       "7       as per your request 'melle melle (oru minnamin...\n",
       "8       winner!! as a valued network customer you have...\n",
       "9       had your mobile 11 months or more? u r entitle...\n",
       "10      i'm gonna be home soon and i don't want to tal...\n",
       "11      six chances to win cash! from 100 to 20,000 po...\n",
       "12      urgent! you have won a 1 week free membership ...\n",
       "13      i've been searching for the right words to tha...\n",
       "14                    i have a date on sunday with will!!\n",
       "15      xxxmobilemovieclub: to use your credit, click ...\n",
       "16                             oh k...i'm watching here:)\n",
       "17      eh u remember how 2 spell his name... yes i di...\n",
       "18      fine if thats the way u feel. thats the way ...\n",
       "19      england v macedonia - dont miss the goals/team...\n",
       "20              is that seriously how you spell his name?\n",
       "21        i‘m going to try for 2 months ha ha only joking\n",
       "22      so ü pay first lar... then when is da stock co...\n",
       "23      aft i finish my lunch then i go str down lor. ...\n",
       "24      ffffffffff. alright no way i can meet up with ...\n",
       "25      just forced myself to eat a slice. i'm really ...\n",
       "26                         lol your always so convincing.\n",
       "27      did you catch the bus ? are you frying an egg ...\n",
       "28      i'm back &amp; we're packing the car now, i'll...\n",
       "29      ahhh. work. i vaguely remember that! what does...\n",
       "                              ...                        \n",
       "5542             armand says get your ass over to epsilon\n",
       "5543               u still havent got urself a jacket ah?\n",
       "5544    i'm taking derek &amp; taylor to walmart, if i...\n",
       "5545        hi its in durban are you still on this number\n",
       "5546           ic. there are a lotta childporn cars then.\n",
       "5547    had your contract mobile 11 mnths? latest moto...\n",
       "5548                   no, i was trying it all weekend ;v\n",
       "5549    you know, wot people wear. t shirts, jumpers, ...\n",
       "5550          cool, what time you think you can get here?\n",
       "5551    wen did you get so spiritual and deep. that's ...\n",
       "5552    have a safe trip to nigeria. wish you happines...\n",
       "5553                          hahaha..use your brain dear\n",
       "5554    well keep in mind i've only got enough gas for...\n",
       "5555    yeh. indians was nice. tho it did kane me off ...\n",
       "5556    yes i have. so that's why u texted. pshew...mi...\n",
       "5557    no. i meant the calculation is the same. that ...\n",
       "5558                               sorry, i'll call later\n",
       "5559    if you aren't here in the next  &lt;#&gt;  hou...\n",
       "5560                    anything lor. juz both of us lor.\n",
       "5561    get me out of this dump heap. my mom decided t...\n",
       "5562    ok lor... sony ericsson salesman... i ask shuh...\n",
       "5563                                  ard 6 like dat lor.\n",
       "5564    why don't you wait 'til at least wednesday to ...\n",
       "5565                                         huh y lei...\n",
       "5566    reminder from o2: to get 2.50 pounds free call...\n",
       "5567    this is the 2nd time we have tried 2 contact u...\n",
       "5568                 will ü b going to esplanade fr home?\n",
       "5569    pity, * was in mood for that. so...any other s...\n",
       "5570    the guy did some bitching but i acted like i'd...\n",
       "5571                           rofl. its true to its name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the words into lower case\n",
    "my_mess=text_message.str.lower()\n",
    "my_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#removing stop words from dataset\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "my_mess=my_mess.apply(lambda x: \" \".join(term for term in x.split() if term not in stop_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stems using Porter Stemmer\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "my_mess=my_mess.apply(lambda x:\" \".join(ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point, crazy.. avail bugi n great wo...\n",
       "1                             ok lar... joke wif u oni...\n",
       "2       free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3               u dun say earli hor... u c alreadi say...\n",
       "4                   nah think goe usf, live around though\n",
       "5       freemsg hey darl 3 week' word back! i'd like f...\n",
       "6       even brother like speak me. treat like aid pat...\n",
       "7       per request 'mell mell (oru minnaminungint nur...\n",
       "8       winner!! valu network custom select receivea £...\n",
       "9       mobil 11 month more? u r entitl updat latest c...\n",
       "10      i'm gonna home soon want talk stuff anymor ton...\n",
       "11      six chanc win cash! 100 20,000 pound txt> csh1...\n",
       "12      urgent! 1 week free membership £100,000 prize ...\n",
       "13      i'v search right word thank breather. promis w...\n",
       "14                                     date sunday will!!\n",
       "15      xxxmobilemovieclub: use credit, click wap link...\n",
       "16                                oh k...i'm watch here:)\n",
       "17      eh u rememb 2 spell name... ye did. v naughti ...\n",
       "18                fine that way u feel. that way gota b\n",
       "19      england v macedonia - dont miss goals/team new...\n",
       "20                                    serious spell name?\n",
       "21                          i‘m go tri 2 month ha ha joke\n",
       "22                   ü pay first lar... da stock comin...\n",
       "23      aft finish lunch go str lor. ard 3 smth lor. u...\n",
       "24                   ffffffffff. alright way meet sooner?\n",
       "25      forc eat slice. i'm realli hungri tho. sucks. ...\n",
       "26                                  lol alway convincing.\n",
       "27      catch bu ? fri egg ? make tea? eat mom' left d...\n",
       "28      i'm back &amp; we'r pack car now, i'll let kno...\n",
       "29           ahhh. work. vagu rememb that! feel like? lol\n",
       "                              ...                        \n",
       "5542                           armand say get ass epsilon\n",
       "5543                 u still havent got urself jacket ah?\n",
       "5544    i'm take derek &amp; taylor walmart, i'm back ...\n",
       "5545                               hi durban still number\n",
       "5546                        ic. lotta childporn car then.\n",
       "5547    contract mobil 11 mnths? latest motorola, noki...\n",
       "5548                                   no, tri weekend ;v\n",
       "5549    know, wot peopl wear. shirts, jumpers, hat, be...\n",
       "5550                           cool, time think get here?\n",
       "5551                    wen get spiritu deep. that' great\n",
       "5552    safe trip nigeria. wish happi soon compani sha...\n",
       "5553                                hahaha..us brain dear\n",
       "5554    well keep mind i'v got enough ga one round tri...\n",
       "5555    yeh. indian nice. tho kane bit he. shud go 4 d...\n",
       "5556           ye have. that' u texted. pshew...miss much\n",
       "5557    no. meant calcul same. &lt;#&gt; unit &lt;#&gt...\n",
       "5558                               sorry, i'll call later\n",
       "5559                   next &lt;#&gt; hour imma flip shit\n",
       "5560                               anyth lor. juz us lor.\n",
       "5561         get dump heap. mom decid come lowes. boring.\n",
       "5562    ok lor... soni ericsson salesman... ask shuhui...\n",
       "5563                                  ard 6 like dat lor.\n",
       "5564                  wait 'til least wednesday see get .\n",
       "5565                                           huh lei...\n",
       "5566    remind o2: get 2.50 pound free call credit det...\n",
       "5567    2nd time tri 2 contact u. u £750 pound prize. ...\n",
       "5568                             ü b go esplanad fr home?\n",
       "5569             pity, * mood that. so...ani suggestions?\n",
       "5570    guy bitch act like i'd interest buy someth els...\n",
       "5571                                      rofl. true name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the each words\n",
    "from nltk.tokenize import word_tokenize\n",
    "#creating the bag of words\n",
    "all_words=[]\n",
    "for message in my_mess:\n",
    "    words=word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "all_words=nltk.FreqDist(all_words)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:9313\n"
     ]
    }
   ],
   "source": [
    "#print totol number of words\n",
    "print(\"Number of words:{}\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use most common words as features\n",
    "word_features=list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "jurong\n",
      "point\n",
      ",\n",
      "crazy..\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "...\n",
      "cine\n",
      "got\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "# The find_features function determined the 1500 words features are contained in the review\n",
    "def find_features(message):\n",
    "    words=word_tokenize(message)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features[word]=(word in words)\n",
    "        \n",
    "    return features\n",
    "features =find_features(processed[0])\n",
    "for key,value in features.items():\n",
    "    if value==True:\n",
    "        print(key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do it for all messages\n",
    "messages=zip(my_mess, y)\n",
    "seed=1\n",
    "np.random.seed=seed\n",
    "#np.random.shuffle(messages)\n",
    "featuresets=[(find_features(text),label) for (text,label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the features dataset into training dataset and testing dataset using Scikit-learn\n",
    "from sklearn import model_selection\n",
    "training,testing=model_selection.train_test_split(featuresets,test_size=0.25,random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "#size of splited datasets\n",
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn classifier with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 98.63603732950466\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow ! Model predict with 98.63% accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets we chack other algorithms with hope of better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define models to train\n",
    "names=[\"KN neighbor\",\"Decision tree\",\"Random forest\",\"Logistic regression\",\"SGD classifier\",\"Naive bayes\",\"SVM linear\"]\n",
    "\n",
    "classifiers=[\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel=\"linear\")\n",
    "    \n",
    "    \n",
    "]\n",
    "models=zip(names,classifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KN neighbor 92.24694903086863\n",
      "Decision tree 96.33883704235463\n",
      "Random forest 97.77458722182341\n",
      "Logistic regression 98.49246231155779\n",
      "SGD classifier 98.06173725771716\n",
      "Naive bayes 97.98994974874373\n",
      "SVM linear 98.63603732950466\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "for name,model in models:\n",
    "    nltk_model=SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy=nltk.classify.accuracy(nltk_model,testing)*100\n",
    "    print(name,accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As observe SVM linear predict with highest accuracy 98.63%  and  KN neighbor predict least accuracy 92.24% in all above algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try ensemble method for more better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble method-volting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# define model to train\n",
    "names=[\"KN neighbors\",\"Decision tree\",\"Random forest\",\"Logistic regression\",\"SGD classifier\",\"Naive bayes\",\"SVM linear\"]\n",
    "classifiers=[\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel=\"linear\")\n",
    "] \n",
    "models=list(zip(names,classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.63603732950466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "nltk_ensemble=SklearnClassifier(VotingClassifier(estimators=models,voting =\"hard\",n_jobs=-1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy=nltk.classify.accuracy(nltk_ensemble,testing)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ensemble method ,we also got 98.63% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#make class label prediction for testing report\n",
    "txt_features,labels=zip(*testing)\n",
    "prediction=nltk_ensemble.classify_many(txt_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      1208\n",
      "          1       0.99      0.91      0.95       185\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1206</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>17</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1206    2\n",
       "       spam        17  168"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print a confusion matrics and classification report\n",
    "print(classification_report(labels,prediction))\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels,prediction),\n",
    "    index=[[\"actual\",\"actual\"],[\"ham\",\"spam\"]],\n",
    "    columns=[[\"predicted\",\"predicted\"],[\"ham\",\"spam\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
